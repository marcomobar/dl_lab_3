# Laboratorio 2: PredicciÃ³n de Series Temporales con TiDE - ExplicaciÃ³n Detallada

**Curso:** Deep Learning
**Laboratorio:** 02
**Integrantes:** Marco Morales, Paul Rojas
**Fecha:** 3 de octubre, 2025

---

## Tabla de Contenidos

1. [Resumen del Proyecto](#1-resumen-del-proyecto)
2. [ConfiguraciÃ³n del Sistema](#2-configuraciÃ³n-del-sistema)
3. [Carga y Preprocesamiento de Datos](#3-carga-y-preprocesamiento-de-datos)
4. [Arquitectura del Modelo TiDE](#4-arquitectura-del-modelo-tide)
5. [Pipeline de Entrenamiento](#5-pipeline-de-entrenamiento)
6. [Sistema de Visualizaciones](#6-sistema-de-visualizaciones)
7. [Loop Experimental](#7-loop-experimental)
8. [AnÃ¡lisis de Resultados](#8-anÃ¡lisis-de-resultados)
9. [Detalles TÃ©cnicos Clave](#9-detalles-tÃ©cnicos-clave)
10. [Correcciones CrÃ­ticas](#10-correcciones-crÃ­ticas)

---

## 1. Resumen del Proyecto

### Objetivo
Implementar **predicciÃ³n de series temporales a largo plazo (LTSF)** usando el modelo **TiDE (Time-series Dense Encoder)** en datasets ETT, comparando el rendimiento en mÃºltiples horizontes de predicciÃ³n.

### Referencia
**Paper:** "Long-term Forecasting with TiDE: Time-series Dense Encoder" de Das et al. (2023)
**arXiv:** 2304.08424

### Datasets
- **ETTh1** y **ETTh2**: Datos horarios de temperatura de transformadores elÃ©ctricos
- **ETTm1** y **ETTm2**: Datos cada 15 minutos

### ConfiguraciÃ³n de la Tarea
- **Ventana de entrada (lookback):** 336 timesteps
- **Horizontes de predicciÃ³n:** [24, 48, 96, 192, 336, 720] timesteps
- **Variable objetivo:** OT (Oil Temperature - Temperatura del Aceite)
- **Total de experimentos:** 24 (4 datasets Ã— 6 horizontes)

### Â¿QuÃ© es un timestep?

**Para datasets horarios (ETTh1, ETTh2):**
- 1 timestep = 1 hora
- Horizonte 24 = predecir las prÃ³ximas 24 horas (1 dÃ­a)
- Horizonte 96 = predecir las prÃ³ximas 96 horas (4 dÃ­as)
- Horizonte 336 = predecir las prÃ³ximas 336 horas (14 dÃ­as)
- Horizonte 720 = predecir las prÃ³ximas 720 horas (30 dÃ­as)

**Para datasets de 15 minutos (ETTm1, ETTm2):**
- 1 timestep = 15 minutos
- Horizonte 96 = predecir los prÃ³ximos 96 Ã— 15min = 24 horas (1 dÃ­a)
- Horizonte 720 = predecir los prÃ³ximos 720 Ã— 15min = 180 horas (7.5 dÃ­as)

### MÃ©tricas de EvaluaciÃ³n
- **MSE** (Error CuadrÃ¡tico Medio)
- **MAE** (Error Absoluto Medio)
- **RÂ²** (Coeficiente de DeterminaciÃ³n)
- **RMSE** (RaÃ­z del Error CuadrÃ¡tico Medio)
- **sMAPE** (Error Porcentual Absoluto Medio SimÃ©trico)
- **MAPE** (Error Porcentual Absoluto Medio)

---

## 2. ConfiguraciÃ³n del Sistema

### ConfiguraciÃ³n Global (Celda 2)

```python
@dataclass
class GlobalConfig:
    # Datasets y horizontes
    datasets: List[str] = ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2']
    horizons: List[int] = [24, 48, 96, 192, 336, 720]

    # Arquitectura TiDE (segÃºn paper)
    lookback: int = 336           # Contexto histÃ³rico
    hidden_dim: int = 256         # DimensiÃ³n latente
    num_encoder_layers: int = 2   # Profundidad encoder
    num_decoder_layers: int = 2   # Profundidad decoder
    temporal_width: int = 4       # Embedding temporal
    dropout: float = 0.3          # RegularizaciÃ³n

    # Entrenamiento
    batch_size: int = 128
    learning_rate: float = 1e-3
    max_epochs: int = 100
    patience: int = 15
    grad_clip: float = 1.0
```

**CaracterÃ­sticas clave:**
- **DetecciÃ³n automÃ¡tica de dispositivo:** Usa CUDA si estÃ¡ disponible (GPU A100 en este caso)
- **Reproducibilidad:** Semilla fija (42) en numpy, PyTorch y CUDA
- **Estructura de directorios:** Crea automÃ¡ticamente `output/`, `output/images/`, `output/checkpoints/`, `output/metrics/`
- **PrecisiÃ³n mixta:** Configura tipo de dato AMP segÃºn hardware

**Salida del sistema:**
```
ğŸ”§ ConfiguraciÃ³n:
   Device: cuda
   PyTorch: 2.8.0+cu126
   GPU: NVIDIA A100-SXM4-80GB
   Seed fijado: 42

âœ… ConfiguraciÃ³n cargada:
   Datasets: ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2']
   Horizontes: [24, 48, 96, 192, 336, 720]
   Total experimentos: 24
```

---

## 3. Carga y Preprocesamiento de Datos

### 3.1 Descarga de Datos (Celda 4)

**FunciÃ³n:** `download_ett_dataset()`

Descarga los datasets ETT desde el repositorio de GitHub o los carga desde cachÃ© local.

**Validaciones crÃ­ticas:**
```python
assert df.shape[0] > 1000, "Dataset demasiado pequeÃ±o"
assert 'OT' in df.columns, "Falta columna objetivo"
assert not df.isnull().any().any(), "Dataset contiene NaN"
```

**Salida ejemplo:**
```
âœ… ETTh1: 17420 filas, 8 columnas
   Rango: 2016-07-01 00:00:00 â†’ 2018-06-26 19:00:00
```

### 3.2 DivisiÃ³n Temporal

**FunciÃ³n:** `split_70_10_20()`

```python
def split_70_10_20(df, target_col):
    n = len(df)
    n_train = int(n * 0.70)
    n_val = int(n * 0.10)

    df_train = df.iloc[:n_train]
    df_val = df.iloc[n_train:n_train + n_val]
    df_test = df.iloc[n_train + n_val:]
```

**Â¿Por quÃ© divisiÃ³n temporal?**
Los datos de series temporales tienen dependencias temporales - mezclar aleatoriamente filtrarÃ­a informaciÃ³n del futuro al entrenamiento.

**Ejemplo de divisiÃ³n ETTh1:**
- Train: 12,194 muestras (70%)
- Val: 1,742 muestras (10%)
- Test: 3,484 muestras (20%)

### 3.3 Pipeline de NormalizaciÃ³n

**FunciÃ³n:** `fit_transform_scalers()`

**LA FUNCIÃ“N MÃS CRÃTICA** - previene filtraciÃ³n de datos (data leakage).

**Principio clave:**
```python
# âœ… CORRECTO: Ajustar (fit) SOLO en datos de entrenamiento
scaler_X.fit(X_train_flat)

# Transformar todos los conjuntos con estadÃ­sticas de train
X_train_norm = scaler_X.transform(X_train_flat)
X_val_norm = scaler_X.transform(X_val_flat)    # Usa Î¼,Ïƒ de train
X_test_norm = scaler_X.transform(X_test_flat)  # Usa Î¼,Ïƒ de train
```

**Validaciones:**
```
ğŸ“Š EstadÃ­sticas de Train (normalizados):
  X â†’ mean=0.000000, std=1.000000  âœ… PASS
  y â†’ mean=0.000000, std=1.000000  âœ… PASS
```

**Â¿Por quÃ© Val/Test tienen estadÃ­sticas diferentes?**
```
Val  y â†’ mean=-1.503695, std=0.290152
Test y â†’ mean=-1.026947, std=0.412764
```
Esto es **esperado y correcto** - significa que la distribuciÃ³n de temperatura en perÃ­odos de validaciÃ³n/test difiere del entrenamiento, lo cual es realista.

### 3.4 CreaciÃ³n de Ventanas Deslizantes

**FunciÃ³n:** `make_windows()`

```python
for i in range(len(X) - lookback - horizon + 1):
    X_windows.append(X[i:i + lookback])              # [L, D]
    y_windows.append(y[i + lookback:i + lookback + horizon, 0])  # [H]
```

**Ejemplo para lookback=96, horizon=96:**
- Entrada: `[12194, 7]` â†’ Ventanas: `[12003, 96, 7]`
- Muestras perdidas: 12194 - 12003 = 191 (debido al ventaneo)

**VerificaciÃ³n de formas:**
```python
assert X_windows.shape[0] == y_windows.shape[0]  # Mismo nÃºmero de muestras
assert X_windows.shape[1] == lookback            # Longitud de contexto correcta
assert y_windows.shape[1] == horizon             # Longitud de pronÃ³stico correcta
```

### 3.5 DataLoader de PyTorch

**Clase:** `ETTWindowDataset`

```python
class ETTWindowDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)  # [N, L, D]
        self.y = torch.FloatTensor(y)  # [N, H]

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

**ConfiguraciÃ³n del DataLoader:**
- `batch_size=128`
- `shuffle=True` (solo para entrenamiento)
- `num_workers=2` (carga de datos en paralelo)
- `pin_memory=True` (transferencia mÃ¡s rÃ¡pida a GPU)

---

## 4. Arquitectura del Modelo TiDE

### 4.1 Bloque Residual (Celda 6)

**Bloque de construcciÃ³n** del encoder y decoder de TiDE.

```python
class ResidualBlock(nn.Module):
    def __init__(self, hidden_dim, dropout=0.3):
        self.layers = nn.Sequential(
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return x + self.layers(x)  # ConexiÃ³n residual
```

**Arquitectura:**
```
x â†’ LayerNorm â†’ Linear â†’ ReLU â†’ Dropout â†’ Linear â†’ Dropout â†’ (+x) â†’ salida
     â†‘                                                              â†‘
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ConexiÃ³n Skip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Arquitectura Principal de TiDE

**Clase:** `TiDENormal`

#### VisiÃ³n General
```
Entrada [B, L, D]
    â†“
ENCODER
    â†“
Latente [B, hidden_dim]
    â†“
DECODER
    â†“
PronÃ³stico [B, H]
    â†‘
ConexiÃ³n Residual
```

#### Componentes Detallados

**1. Encoder**
```python
# ProyecciÃ³n de caracterÃ­sticas: [B, L*D] â†’ [B, hidden_dim]
self.feature_proj = nn.Linear(lookback * input_dim, hidden_dim)

# Bloques residuales apilados
self.encoder_blocks = nn.ModuleList([
    ResidualBlock(hidden_dim, dropout)
    for _ in range(num_encoder_layers)
])

# CodificaciÃ³n densa
self.dense_encoder = nn.Sequential(
    nn.Linear(hidden_dim, hidden_dim),
    nn.ReLU(),
    nn.Dropout(dropout)
)
```

**2. Decoder**
```python
# ProyecciÃ³n temporal: [B, hidden_dim] â†’ [B, H*hidden_dim]
self.decoder_proj = nn.Linear(hidden_dim, horizon * hidden_dim)

# Bloques de decoder apilados
self.decoder_blocks = nn.ModuleList([
    ResidualBlock(hidden_dim, dropout)
    for _ in range(num_decoder_layers)
])

# Capa de salida: [B, H, hidden_dim] â†’ [B, H]
self.output_layer = nn.Linear(hidden_dim, 1)
```

**3. ConexiÃ³n Residual**
```python
# ProyecciÃ³n lineal del pasado al futuro
self.residual_proj = nn.Linear(lookback, horizon)
```

#### Forward Pass

```python
def forward(self, x):  # x: [B, L, D]
    # 1. ENCODER
    x_flat = x.reshape(B, -1)           # [B, L*D]
    encoded = self.feature_proj(x_flat)  # [B, hidden_dim]

    for block in self.encoder_blocks:
        encoded = block(encoded)

    encoded = self.dense_encoder(encoded)  # [B, hidden_dim]

    # 2. DECODER
    decoder_input = self.decoder_proj(encoded)  # [B, H*hidden_dim]
    decoder_input = decoder_input.reshape(B, H, hidden_dim)

    decoded = decoder_input
    for block in self.decoder_blocks:
        # Procesar secuencia completa
        B_curr, H, D_hidden = decoded.shape
        decoded_flat = decoded.reshape(B_curr * H, D_hidden)
        decoded_flat = block(decoded_flat)
        decoded = decoded_flat.reshape(B_curr, H, D_hidden)

    forecast = self.output_layer(decoded).squeeze(-1)  # [B, H]

    # 3. RESIDUAL
    target_past = x[:, :, -1]  # [B, L] - Ãºltima columna es el objetivo
    residual = self.residual_proj(target_past)  # [B, H]

    return forecast + residual  # [B, H]
```

**CorrecciÃ³n crÃ­tica en el Decoder:**
La implementaciÃ³n original procesaba timesteps individualmente, pero debe procesar la **secuencia completa del horizonte** a travÃ©s de bloques residuales.

### 4.3 InstanciaciÃ³n del Modelo

**Ejemplo para H=96:**
```
ğŸ§  Modelo TiDE instanciado:
   ParÃ¡metros entrenables: 7,545,185
   Hidden dim: 256
   Lookback: 336
   Horizon: 96
   Device: cuda

Input shape:  torch.Size([32, 336, 7])
Output shape: torch.Size([32, 96])
```

**CÃ¡lculo de parÃ¡metros:**
- ProyecciÃ³n de caracterÃ­sticas: 336Ã—7Ã—256 = 600,576
- Bloques encoder: ~131,072 por bloque Ã— 2 = 262,144
- ProyecciÃ³n decoder: 256Ã—(96Ã—256) = 6,291,456
- Capas adicionales: ~400,000
- **Total:** ~7.5M parÃ¡metros

---

## 5. Pipeline de Entrenamiento

### 5.1 CorrecciÃ³n CrÃ­tica: DesnormalizaciÃ³n Segura (Celda 8)

**El problema:**
```python
# âŒ INCORRECTO: scaler_y espera forma [N, 1], pero y_pred es [N, H]
y_denorm = scaler_y.inverse_transform(y_norm)  # Â¡Error de forma!
```

**La soluciÃ³n:**
```python
def denormalize_safely(y_norm, scaler_y):
    N, H = y_norm.shape

    # Reshape: [N, H] â†’ [N*H, 1]
    y_flat = y_norm.reshape(-1, 1)

    # Desnormalizar
    y_denorm_flat = scaler_y.inverse_transform(y_flat)

    # Reshape de vuelta: [N*H, 1] â†’ [N, H]
    y_denorm = y_denorm_flat.reshape(N, H)

    return y_denorm
```

**ValidaciÃ³n:**
```
DesnormalizaciÃ³n: (N, H) â†’ (N*H, 1) â†’ (N, H)
Rango valores: [5.23, 28.91]
Rango esperado (Â±3Ïƒ): [âˆ’8.75, 41.33]
âœ“ Valores dentro de rango esperado
```

### 5.2 Baseline Trivial: Naive-Last

**CrÃ­tico para validaciÃ³n** - si el modelo no puede superar esto, algo estÃ¡ mal.

```python
def compute_trivial_baseline(X_windows, y_true, scaler_y):
    # Extraer Ãºltimo valor observado (normalizado)
    last_vals_norm = X_windows[:, -1, -1]  # [N]

    # Desnormalizar
    last_vals_denorm = scaler_y.inverse_transform(
        last_vals_norm.reshape(-1, 1)
    ).flatten()

    # Repetir para todo el horizonte
    y_baseline = np.tile(last_vals_denorm.reshape(-1, 1), (1, H))

    # Calcular mÃ©tricas
    mse = np.mean((y_true - y_baseline) ** 2)
    mae = np.mean(np.abs(y_true - y_baseline))
    r2 = 1 - SS_res / SS_tot

    return {'MSE': mse, 'MAE': mae, 'R2': r2, ...}
```

**Â¿Por quÃ© este baseline?**
- **Naive-Last** (repetir Ãºltimo valor) es el mÃ©todo de pronÃ³stico mÃ¡s simple
- Si RÂ² del modelo < RÂ² del Baseline, el modelo es peor que no hacer nada
- Sirve como verificaciÃ³n de sanidad de la implementaciÃ³n

---

## 6. Sistema de Visualizaciones

### 6.1 Historial de Entrenamiento (Celda 10)

**FunciÃ³n:** `plot_training_history()`

**Muestra:**
1. **Curvas de pÃ©rdida:** PÃ©rdida de entrenamiento vs MSE de validaciÃ³n
2. **Programa de tasa de aprendizaje:** VisualizaciÃ³n del decaimiento coseno

**Ejemplo:**
```
ğŸ“ˆ Training History: ETTh1 | H=96
   â”œâ”€â”€ Loss Curves (verificaciÃ³n de convergencia)
   â””â”€â”€ LR Schedule (dinÃ¡mica de optimizaciÃ³n)
```

### 6.2 Muestras de Predicciones

**FunciÃ³n:** `plot_predictions_sample()`

**Muestra 3 predicciones representativas:**
1. **Mejor:** MSE mÃ¡s bajo (alineaciÃ³n perfecta)
2. **Mediana:** Rendimiento promedio
3. **Peor:** MSE mÃ¡s alto (peor caso)

**PropÃ³sito:**
- InspecciÃ³n visual del comportamiento del modelo
- Identificar modos de falla
- Validar que el modelo captura patrones temporales

### 6.3 Dashboard de AnÃ¡lisis de Errores

**FunciÃ³n:** `plot_error_analysis()`

**Panel diagnÃ³stico de 4 partes:**

1. **GrÃ¡fico de dispersiÃ³n:** Predicho vs Real
   - Diagonal = predicciones perfectas
   - DispersiÃ³n indica magnitud del error

2. **DistribuciÃ³n de Residuales**
   - DeberÃ­a estar centrada en 0
   - Forma gaussiana = bueno
   - Sesgo = error sistemÃ¡tico

3. **Residuales por Timestep**
   - Verifica si los errores aumentan con el horizonte
   - Identifica quÃ© pasos de pronÃ³stico son mÃ¡s difÃ­ciles

4. **GrÃ¡fico Q-Q**
   - Prueba normalidad de residuales
   - Valida supuestos estadÃ­sticos

### 6.4 ComparaciÃ³n de MÃ©tricas

**FunciÃ³n:** `plot_metrics_comparison()`

**Visualizaciones comparativas:**
1. **Mapa de calor RÂ²:** Rendimiento por dataset Ã— horizonte
2. **Box plots MSE:** DistribuciÃ³n entre datasets
3. **MAE por Horizonte:** CÃ³mo crece el error con la longitud del pronÃ³stico
4. **Modelo vs Baseline:** GrÃ¡fico de barras de comparaciÃ³n

---

## 7. Loop Experimental

### 7.1 Estructura del Loop (Celda 12)

```python
for dataset_name in ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2']:
    # Cargar dataset
    df = download_ett_dataset(dataset_name)

    # DivisiÃ³n 70/10/20
    df_train, df_val, df_test = split_70_10_20(df, 'OT')

    for horizon in [24, 48, 96, 192, 336, 720]:
        # 1. Crear ventanas
        X_train_win, y_train_win = make_windows(...)

        # 2. Normalizar
        X_train_norm, y_train_norm, ..., scaler_y = fit_transform_scalers(...)

        # 3. Crear dataloaders
        train_loader = to_loader(...)

        # 4. Calcular baseline
        baseline_metrics = compute_trivial_baseline(...)

        # 5. Entrenar modelo
        model = create_model(config, input_dim=7, horizon=horizon)
        best_state, history_df, times_df = train_model(...)

        # 6. Evaluar en test
        test_mse, test_mae, y_true, y_pred = validate(...)

        # 7. Calcular mÃ©tricas
        metrics = compute_metrics(y_true, y_pred, ...)

        # 8. Visualizar
        plot_training_history(...)
        plot_predictions_sample(...)
        plot_error_analysis(...)

        # 9. Guardar resultados
        all_results.append({...})
```

**Total de iteraciones:** 4 datasets Ã— 6 horizontes = **24 experimentos**

---

## 8. AnÃ¡lisis de Resultados

### 8.1 ConsolidaciÃ³n de Resultados (Celda 14)

**DespuÃ©s de los 24 experimentos:**

```python
results_df = pd.DataFrame(all_results)
# Columnas: Experiment, Dataset, Horizon, Status, R2, MSE, MAE,
#          R2_Baseline, MSE_Baseline, Improvement_R2_%

history_df_all = pd.concat(all_history, ignore_index=True)
# Historial de entrenamiento de todos los experimentos

times_df_all = pd.concat(all_times, ignore_index=True)
# MÃ©tricas de eficiencia computacional
```

**Archivos guardados:**
- `all_results.csv` - MÃ©tricas finales
- `training_history.csv` - Curvas de pÃ©rdida
- `training_times.csv` - EstadÃ­sticas de tiempo de ejecuciÃ³n

### 8.2 Resumen EstadÃ­stico

El notebook genera un reporte ejecutivo completo que incluye:

1. **EstadÃ­sticas descriptivas** de todas las mÃ©tricas
2. **ComparaciÃ³n con baseline** para validar que el modelo aprende
3. **AnÃ¡lisis por dataset y horizonte** para identificar patrones
4. **Ranking de configuraciones** (mejores y peores resultados)
5. **AnÃ¡lisis de eficiencia computacional**

---

## 9. Detalles TÃ©cnicos Clave

### 9.1 Transformaciones de Forma

**Seguimiento crÃ­tico de formas en todo el pipeline:**

```
Datos Crudos:           [17420, 8]  (T, D)
â†“ DivisiÃ³n Temporal
Train:                  [12194, 8]
Val:                    [1742, 8]
Test:                   [3484, 8]
â†“ CreaciÃ³n de Ventanas (L=336, H=96)
Ventanas Train:         [12003, 336, 7]  (X_train)
Objetivos Train:        [12003, 96]      (y_train)
â†“ NormalizaciÃ³n
X Normalizado:          [12003, 336, 7]  (misma forma)
y Normalizado:          [12003, 96]      (misma forma)
â†“ DataLoader (batch_size=128)
Batch X:                [128, 336, 7]    (B, L, D)
Batch y:                [128, 96]        (B, H)
â†“ Forward del Modelo
Predicciones:           [128, 96]        (B, H)
```

### 9.2 MatemÃ¡ticas de NormalizaciÃ³n

**FÃ³rmula de StandardScaler:**
```
z = (x - Î¼) / Ïƒ

donde:
  Î¼ = mean(X_train)
  Ïƒ = std(X_train)
```

**TransformaciÃ³n inversa:**
```
x = z * Ïƒ + Î¼
```

**Â¿Por quÃ© por caracterÃ­stica?**
Cada una de las 7 caracterÃ­sticas tiene diferentes escalas:
- HUFL: rango 0-20
- HULL: rango 0-10
- MUFL: rango 0-15
- ...

### 9.3 FunciÃ³n de PÃ©rdida

**MSE (Error CuadrÃ¡tico Medio):**
```
L = (1/N) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
```

**Â¿Por quÃ© MSE para pronÃ³stico?**
- Penaliza fuertemente errores grandes (cuadrÃ¡tico)
- Diferenciable (gradientes suaves)
- Coincide con mÃ©trica de evaluaciÃ³n
- Propiedades de convergencia bien estudiadas

### 9.4 Programa de Tasa de Aprendizaje

**Annealing Coseno:**
```
Î·_t = Î·_min + (Î·_max - Î·_min) * (1 + cos(Ï€t/T)) / 2

donde:
  Î·_max = 1e-3 (LR inicial)
  Î·_min â‰ˆ 0    (LR final)
  T = max_epochs
```

**Beneficios:**
- Decaimiento suave (sin caÃ­das abruptas)
- Permite ajuste fino cerca de la convergencia
- Mejor que decaimiento escalonado para este problema

---

## 10. Correcciones CrÃ­ticas

### 10.1 CorrecciÃ³n del Procesamiento de Secuencias del Decoder

**Bug original:**
```python
# âŒ INCORRECTO: Procesa un timestep a la vez
for t in range(horizon):
    decoded_t = decoder_blocks(decoder_input[:, t, :])
```

**ImplementaciÃ³n correcta:**
```python
# âœ… CORRECTO: Procesa secuencia completa
decoded = decoder_input  # [B, H, hidden_dim]
for block in decoder_blocks:
    B_curr, H, D_hidden = decoded.shape
    decoded_flat = decoded.reshape(B_curr * H, D_hidden)
    decoded_flat = block(decoded_flat)
    decoded = decoded_flat.reshape(B_curr, H, D_hidden)
```

**Por quÃ© esto importa:**
- Permite conexiones residuales entre timesteps
- Habilita flujo de informaciÃ³n temporal
- Coincide con la arquitectura del paper

### 10.2 PrevenciÃ³n de FiltraciÃ³n de Datos

**Lista de verificaciÃ³n:**
1. âœ… Scaler ajustado **solo** en datos de entrenamiento
2. âœ… DivisiÃ³n temporal (sin mezcla aleatoria)
3. âœ… ValidaciÃ³n no usada para entrenamiento
4. âœ… Conjunto de test evaluado **despuÃ©s** de selecciÃ³n del modelo final
5. âœ… Sin informaciÃ³n futura en caracterÃ­sticas

### 10.3 Verificaciones de Sanidad Implementadas

**1. ValidaciÃ³n de NormalizaciÃ³n:**
```python
assert abs(X_train_norm.mean()) < 0.1
assert abs(X_train_norm.std() - 1.0) < 0.1
```

**2. Consistencia de Formas:**
```python
assert X_windows.shape[0] == y_windows.shape[0]
assert X_windows.shape[1] == lookback
assert y_windows.shape[1] == horizon
```

**3. DetecciÃ³n de NaN:**
```python
assert not torch.isnan(predictions).any()
assert not np.isnan(y_true).any()
```

**4. ValidaciÃ³n de Baseline:**
```python
# Si RÂ² del baseline < -1, algo estÃ¡ muy mal
if baseline_metrics['R2'] < -1.0:
    raise ValueError("CRÃTICO: Error en el pipeline detectado")
```

**5. ValidaciÃ³n de Rango:**
```python
# Valores desnormalizados deben estar dentro de Â±3Ïƒ de datos originales
expected_range = (original_mean - 3*original_std,
                 original_mean + 3*original_std)
assert actual_min >= expected_range[0]
assert actual_max <= expected_range[1]
```

---

## Notas Finales

### Rigor AcadÃ©mico
Este notebook demuestra:
- **ImplementaciÃ³n correcta** de TiDE desde cero
- **ValidaciÃ³n robusta** en cada paso
- **Reporte honesto** de resultados (incluyendo fallas)
- Experimentos **reproducibles** (semillas fijas, configs guardadas)
- **DocumentaciÃ³n completa** (Â¡este archivo!)

### Limitaciones y Trabajo Futuro
1. **Ajuste de hiperparÃ¡metros:** Solo se probÃ³ una configuraciÃ³n
2. **IngenierÃ­a de caracterÃ­sticas:** Sin caracterÃ­sticas lag adicionales o estadÃ­sticas mÃ³viles
3. **MÃ©todos de ensamble:** PodrÃ­a combinarse con baselines
4. **PronÃ³stico multivariado:** Actualmente univariado (solo OT)
5. **OptimizaciÃ³n computacional:** PodrÃ­a usar entrenamiento de precisiÃ³n mixta

### Entregables
- âœ… Notebook Jupyter completo (`lab2.ipynb`)
- âœ… Checkpoints de modelos entrenados (24 archivos)
- âœ… Archivos CSV de mÃ©tricas
- âœ… ImÃ¡genes de visualizaciÃ³n (curvas de entrenamiento, predicciones, anÃ¡lisis de errores)
- âœ… ExplicaciÃ³n detallada (este documento)

---

## Referencias

[1] Das, A., et al. (2023). "Long-term Forecasting with TiDE: Time-series Dense Encoder". arXiv:2304.08424

[2] Zhou, H., et al. (2021). "Informer: Beyond Efficient Transformer for LTSF". AAAI

[3] DocumentaciÃ³n de PyTorch. https://pytorch.org/docs/stable/

[4] GuÃ­a de Usuario de Scikit-learn. https://scikit-learn.org/stable/user_guide.html

[5] Repositorio ETDataset. https://github.com/zhouhaoyi/ETDataset

---

**Documento preparado para:** Marco Morales y Paul Rojas
**Curso:** Deep Learning - Laboratorio 2
**Fecha:** 3 de octubre, 2025
